{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liberires "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"\n",
    "Quantum computing, a burgeoning frontier in technological's advancement, \n",
    "promises to revolutionize industries from pharmaceuticals to finance. \n",
    "Leveraging the principles of quantum mechanics.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(corpus)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we created a empty list called tokenized_sentences and then run a loop for the function as earlier when we word_tokenize(sentences) we were getting an error.\n",
    "\n",
    "tokenized_sentences = []\n",
    "for sentence in sentences:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokenized_sentences.append(tokens)\n",
    "\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_tokenize_sentence = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = wordpunct_tokenize(sentence)\n",
    "    word_tokenize_sentence.append(tokens)\n",
    "\n",
    "print(word_tokenize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TreebankWordToken=[]\n",
    "\n",
    "for sentence in sentences:\n",
    "    token= TreebankWordTokenizer().tokenize(sentence)\n",
    "    TreebankWordToken.append(token)\n",
    "\n",
    "print(TreebankWordToken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list = [\"going\", \"wenting\",\"went\", \"gones\", \"seeing\", \"saws\", \"seen\", \"eat\", \"ate\", \"eaten\", \"run\", \"ran\", \"run\", \"swim\", \"swam\", \"swum\", \"give\", \"gave\", \"given\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer= PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in verb_list:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer= RegexpStemmer('(ing|s|ed)$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer= RegexpStemmer('(ing|s|ed)$', '')\n",
    "words = [\"running\", \"jumps\", \"played\"]\n",
    "\n",
    "stemmed_words = []\n",
    "for word in words:\n",
    "    stemmed_word = reg_stemmer.tokenize(word)\n",
    "    stemmed_words.append(stemmed_word)\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowball Stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow.stem('congratulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow.stem('History')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    snoww=snow.stem(word)\n",
    "\n",
    "    print(snoww)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming can't be used for chatbot use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmitization \n",
    "\n",
    "It gives a root words like eatings ---> eat. not trimmming the word just finding hte root word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('congratulations',pos='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listt = []\n",
    "for word in words:\n",
    "    tokennn=lemmatizer.lemmatize(word,pos='v')\n",
    "    listt.append(tokennn)\n",
    "    print(tokennn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=\"\"\"\n",
    "The quick brown fox jumps over the lazy dog. The lazy dog sleeps under the tree. The quick brown fox and the lazy dog are friends. They play together every day. The fox is quick and agile, while the dog is lazy and loves to nap. One day, the fox decided to play a trick on the dog. He hid the dog's favorite toy, a squeaky ball, under the tree. The dog searched everywhere for his ball, but he couldn't find it. He started to whine and bark. The fox watched from a distance, amused by the dog's predicament. Finally, the fox revealed the hiding place of the ball. The dog was overjoyed to find his toy and wagged his tail happily. The fox and the dog continued to be friends, playing together and enjoying each other's company. The fox learned that playing tricks on his friend was not always a good idea, and the dog learned to be more patient and resourceful.\n",
    "\n",
    "The fox and the dog lived in a peaceful forest. They were surrounded by tall trees, lush greenery, and a babbling brook. One day, they heard a strange noise coming from the woods. They decided to investigate. As they ventured deeper into the forest, they came across a small clearing. In the center of the clearing was a tiny cottage with a thatched roof. Smoke was rising from the chimney, and a faint aroma of freshly baked bread filled the air. The fox and the dog approached the cottage cautiously. They peeked inside and saw a little old woman sitting by the window, knitting a sweater. The woman looked up and smiled at them. She invited them inside to join her for tea and biscuits. The fox and the dog were hesitant at first, but they eventually accepted the invitation. The woman was kind and welcoming, and she told them stories of her life in the forest. They spent the afternoon laughing and talking. When it was time to leave, the woman gave them a basket of fresh bread and a jar of honey. The fox and the dog thanked her and continued on their way. They had made a new friend and discovered a hidden gem in their forest home.\n",
    "\n",
    "The fox and the dog continued their adventures in the forest. They explored hidden caves, climbed tall trees, and swam in the crystal-clear river. They learned about the different plants and animals that lived in the forest. They also discovered a secret waterfall hidden behind a curtain of vines. One day, while exploring the waterfall, they stumbled upon a hidden treasure. It was a chest filled with gold coins and precious jewels. The fox and the dog were amazed. They had never seen so much wealth before. They decided to share the treasure equally. They used the treasure to buy food, toys, and other things they needed. They also donated some of the treasure to a local charity. The fox and the dog were grateful for their good fortune and continued to live happily in the forest.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.sent_tokenize(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[186], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentences)):\n\u001b[1;32m----> 4\u001b[0m     word\u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(\u001b[43msentence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      5\u001b[0m     stemmed_words \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mif\u001b[39;00m sentence \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m      6\u001b[0m     tokenized_sentences\u001b[38;5;241m.\u001b[39mappend(stemmed_words)\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in range(len(sentences)):\n",
    "    word= nltk.word_tokenize(sentence[i])\n",
    "    stemmed_words = [stemmer.stem(sentence) for sentence in sentences if sentence not in stop_words]\n",
    "    tokenized_sentences.append(stemmed_words)\n",
    "\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
